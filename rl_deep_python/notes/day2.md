# Day2: 強化学習の解放(1): 環境から計画を立てる.
## (p29)
+ 計画を立てるには，`状態評価`と`戦略`の学習がいる -> 実際に学習をしていく
+ `価値`をMDPの環境に即した形にしていく.
+ `価値の定義`，`状態評価の学習`，`戦略の学習`
+ `DP`: 遷移関数と報酬関数が明らかな時に使える -> これらをベースに学習をする手法を`モデル(環境)ベース` (既知でなくてもよく，推定しながらでも学習できる)
+ モデル(遷移関数，報酬関数)を使わない -> `モデリフリー`

### この章の要点
+ 行動の指標となる`価値`の定義
+ `状態評価`，`戦略`をDPで学習する手法と実装
+ `モデルベース`と`モデルフリー`の違い

## 2.1 価値の定義と算出: Bellman Equation
+ 価値に2つの問題がある.
+ 将来の即時報酬の値($r_{t+1}$)が判明している必要がある点
+ その報酬が必ず得られるとしている点
-> この`価値`をそのまま計算するのは困難であるので，この2つを解消せなあかん  

### 1点目: 将来の即時報酬が判明してないとだめ
+ 式を再帰的にする方法で解決する -> 計算を持ち越せる
+ $G_{t+1}$には，いったん適当な値を入れて，$G_{t}$を計算する. -> 計算時点で将来の即時報酬が判明している必要がなくなる
+ DPでは，$G_{t+1}$には，過去の計算(キャッシュ)を使って計算する -> `メモ化`

### 2点目: その報酬を必ず貰えるとしている点
+ 即時報酬に確率をかける -> 期待値を計算していることにする
エージェントの行動方法
+ 保持している戦略に基づき行動 -> `Policyベース`
+ 常に`価値`が最大になる行動を選択 -> `Valueベース`: `行動の評価方法`のみ学習して評価 = 行動選択


## 2.2 DPによる状態評価の学習: Value Iteration
+ Valueベース: 各状態の価値を算出し，値が最も高い状態に遷移するように行動
+ DPによって，各状態の価値を算出する方法を，`価値反復法(Value Iteration)`
+ 正確な値へ近づいたかは，|V_{i+1}(s) - V_{i}(s)|が閾値を下回ったかで判定


## 2.3 DPによる戦略の学習: Policy Iteration
